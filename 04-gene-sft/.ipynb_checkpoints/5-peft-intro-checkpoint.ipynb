{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963e9ae0-ac68-44be-8c7d-fb9842784362",
   "metadata": {},
   "source": [
    "# 4.5 peftç®€ä»‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4288594-c676-4369-aca1-730446f293d7",
   "metadata": {},
   "source": [
    "## peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b82c4-d484-4c15-a600-03c3b51367ec",
   "metadata": {},
   "source": [
    "**PEFT**ï¼ˆParameter-Efficient Fine-Tuningï¼Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰æ˜¯ä¸€ç§ä¼˜åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨ä»¥æœ€å°çš„å‚æ•°æ›´æ–°å®ç°å¯¹å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ GPTã€BERT ç­‰ï¼‰çš„å¾®è°ƒã€‚PEFT æŠ€æœ¯é€šè¿‡å‡å°‘å¾®è°ƒæ‰€éœ€çš„å‚æ•°é‡ï¼Œæ˜¾è‘—é™ä½äº†å­˜å‚¨å’Œè®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«é€‚åˆèµ„æºå—é™çš„åœºæ™¯å’Œé¢†åŸŸç‰¹å®šä»»åŠ¡çš„å®šåˆ¶åŒ–ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **1. æ ¸å¿ƒæ€æƒ³**\n",
    "ä¼ ç»Ÿçš„å¾®è°ƒæ–¹å¼éœ€è¦æ›´æ–°æ•´ä¸ªé¢„è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼ŒPEFT æŠ€æœ¯é€šè¿‡åªè°ƒæ•´å°‘é‡çš„å‚æ•°ï¼ˆå¦‚ç‰¹å®šå±‚æˆ–é¢å¤–æ·»åŠ çš„å°å‹æ¨¡å—ï¼‰å®ç°å¾®è°ƒç›®æ ‡ï¼Œå¤§å¹…å‡å°‘äº†è®­ç»ƒå¼€é”€å’Œå­˜å‚¨éœ€æ±‚ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **2. å¸¸è§çš„ PEFT æ–¹æ³•**\n",
    "\n",
    "#### **ï¼ˆ1ï¼‰Adapter æ¨¡å‹**\n",
    "- åœ¨æ¯ä¸€å±‚ Transformer çš„è¾“å‡ºä¸­æ’å…¥å°å‹é€‚é…å™¨æ¨¡å—ï¼Œä»…è®­ç»ƒé€‚é…å™¨æ¨¡å—çš„å‚æ•°ã€‚\n",
    "- åŸå§‹æ¨¡å‹å‚æ•°ä¿æŒå†»ç»“ä¸å˜ã€‚\n",
    "- ä¼˜ç‚¹ï¼šé€‚é…å™¨æ¨¡å—å‚æ•°é‡å°ï¼Œèƒ½é€‚åº”ä¸åŒä»»åŠ¡ã€‚\n",
    "\n",
    "ç¤ºä¾‹æ–¹æ³•ï¼š\n",
    "- **AdapterFusion**\n",
    "- **MAD-X**\n",
    "\n",
    "---\n",
    "\n",
    "#### **ï¼ˆ2ï¼‰Prefix Tuning**\n",
    "- åœ¨ Transformer çš„è¾“å…¥å‰æ·»åŠ ä¸€ç»„å¯å­¦ä¹ çš„å‰ç¼€å‘é‡ï¼Œè¿™äº›å‰ç¼€ä¸æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶äº¤äº’ã€‚\n",
    "- åªè°ƒæ•´å‰ç¼€å‘é‡çš„å‚æ•°ï¼Œè€Œä¸æ›´æ–°åŸå§‹æ¨¡å‹ã€‚\n",
    "- ä¼˜ç‚¹ï¼šå¯¹ç”Ÿæˆä»»åŠ¡æ•ˆæœæ˜¾è‘—ï¼Œå‚æ•°é‡è¿›ä¸€æ­¥å‡å°‘ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### **ï¼ˆ3ï¼‰LoRAï¼ˆLow-Rank Adaptationï¼‰**\n",
    "- å°†é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„éƒ¨åˆ†æƒé‡åˆ†è§£ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µï¼Œä»…è°ƒæ•´è¿™äº›ä½ç§©çŸ©é˜µçš„å‚æ•°ã€‚\n",
    "- åŸå§‹æƒé‡ä¿æŒå†»ç»“çŠ¶æ€ã€‚\n",
    "- ä¼˜ç‚¹ï¼šå‚æ•°é‡æå°ï¼Œè®¡ç®—é«˜æ•ˆã€‚\n",
    "  \n",
    "---\n",
    "\n",
    "#### **ï¼ˆ4ï¼‰Prompt Tuning**\n",
    "- åœ¨è¾“å…¥æ–‡æœ¬ä¸­æ·»åŠ å¯å­¦ä¹ çš„æç¤ºï¼ˆPromptï¼‰ã€‚\n",
    "- é€‚åˆ NLP ä»»åŠ¡ä¸­çš„æ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ç­‰ã€‚\n",
    "- ä¼˜ç‚¹ï¼šå®ç°ç®€å•ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰æ¡†æ¶ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **3. PEFT çš„ä¼˜åŠ¿**\n",
    "\n",
    "1. **æ˜¾è‘—å‡å°‘å‚æ•°æ›´æ–°é‡**ï¼š\n",
    "   - å¾®è°ƒä¼ ç»Ÿçš„å¤§æ¨¡å‹ï¼ˆå¦‚ GPT-3ï¼‰éœ€è¦æ›´æ–°æ•°ç™¾äº¿å‚æ•°ï¼Œè€Œ PEFT ä»…éœ€æ›´æ–°ç™¾ä¸‡çº§åˆ«ç”šè‡³æ›´å°‘çš„å‚æ•°ã€‚\n",
    "\n",
    "2. **é«˜æ•ˆå­˜å‚¨**ï¼š\n",
    "   - æ¯ä¸ªä»»åŠ¡çš„å¾®è°ƒç»“æœåªéœ€å­˜å‚¨å°‘é‡é¢å¤–å‚æ•°ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹ã€‚\n",
    "\n",
    "3. **é€‚ç”¨å¤šä»»åŠ¡**ï¼š\n",
    "   - åŒä¸€é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥é€šè¿‡ä¸åŒçš„ PEFT æ¨¡å—é€‚é…å¤šä¸ªä»»åŠ¡ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚\n",
    "\n",
    "4. **é™ä½è®¡ç®—å¼€é”€**ï¼š\n",
    "   - è®­ç»ƒæ‰€éœ€çš„å†…å­˜å’Œè®¡ç®—æ˜¾è‘—å‡å°‘ï¼Œé€‚åˆèµ„æºæœ‰é™çš„ç¯å¢ƒã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **4. åº”ç”¨åœºæ™¯**\n",
    "\n",
    "1. **é¢†åŸŸç‰¹å®šä»»åŠ¡**ï¼š\n",
    "   - åŒ»ç–—ã€æ³•å¾‹ã€é‡‘èç­‰é¢†åŸŸå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "\n",
    "2. **å¤šä»»åŠ¡å­¦ä¹ **ï¼š\n",
    "   - é€‚é…å¤šä¸ªä»»åŠ¡ï¼Œå¤ç”¨åŒä¸€æ¨¡å‹çš„é¢„è®­ç»ƒæƒé‡ã€‚\n",
    "\n",
    "3. **èµ„æºå—é™åœºæ™¯**ï¼š\n",
    "   - ç§»åŠ¨è®¾å¤‡ã€è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ¨¡å‹éƒ¨ç½²ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Hugging Face PEFT åº“**\n",
    "\n",
    "Hugging Face æä¾›äº†ä¸“é—¨çš„ PEFT åº“ï¼Œæ”¯æŒå¤šç§å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼š\n",
    "- **å®‰è£…**ï¼š\n",
    "  ```bash\n",
    "  pip install peft\n",
    "  ```\n",
    "- **ä½¿ç”¨ LoRA å¾®è°ƒç¤ºä¾‹**ï¼š\n",
    "  ```python\n",
    "  from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "  from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "  # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "  model_name = \"gpt2\"\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  # é…ç½® LoRA\n",
    "  lora_config = LoraConfig(\n",
    "      task_type=TaskType.CAUSAL_LM,\n",
    "      r=8,\n",
    "      lora_alpha=32,\n",
    "      target_modules=[\"q_proj\", \"v_proj\"],\n",
    "      lora_dropout=0.1,\n",
    "      bias=\"none\"\n",
    "  )\n",
    "\n",
    "  # ä½¿ç”¨ LoRA å¾®è°ƒæ¨¡å‹\n",
    "  model = get_peft_model(model, lora_config)\n",
    "  model.print_trainable_parameters()\n",
    "\n",
    "  # å¾®è°ƒä»£ç ...\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. PEFT çš„å±€é™æ€§**\n",
    "1. **ç‰¹å®šä»»åŠ¡é™åˆ¶**ï¼š\n",
    "   - åœ¨ä¸€äº›å¤æ‚ä»»åŠ¡ä¸­ï¼ŒPEFT æ–¹æ³•å¯èƒ½ä¸å¦‚å…¨é‡å¾®è°ƒæ•ˆæœå¥½ã€‚\n",
    "\n",
    "2. **éœ€è¦è®¾è®¡åˆé€‚çš„æ¨¡å—**ï¼š\n",
    "   - ä¸åŒä»»åŠ¡éœ€è¦é€‰æ‹©å’Œè®¾è®¡åˆé€‚çš„ PEFT æŠ€æœ¯ã€‚\n",
    "\n",
    "3. **ä¸æ¨¡å‹æ¶æ„ç›¸å…³**ï¼š\n",
    "   - PEFT æŠ€æœ¯å¯èƒ½éœ€è¦å¯¹æ¨¡å‹æ¶æ„è¿›è¡Œä¸€å®šç¨‹åº¦çš„ä¿®æ”¹ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **7. å°ç»“**\n",
    "PEFT æ˜¯ä¸€ä¸ªæå…·æ½œåŠ›çš„æŠ€æœ¯ï¼Œç‰¹åˆ«é€‚åˆåœ¨æœ‰é™èµ„æºä¸‹å¯¹å¤§æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®ƒåœ¨è®¸å¤šé¢†åŸŸå’Œä»»åŠ¡ä¸­å·²æ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ•ˆæœï¼Œä¾‹å¦‚ LoRA å’Œ Adapter æ¨¡å‹å·²ç»æˆä¸ºé«˜æ•ˆå¾®è°ƒçš„ä¸»æµæ–¹æ³•ã€‚\n",
    "\n",
    "å¦‚æœæ‚¨éœ€è¦å®ç°é«˜æ•ˆå¾®è°ƒï¼Œå¯ä»¥ç»“åˆ Hugging Face çš„ PEFT åº“å¿«é€Ÿä¸Šæ‰‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b2631-c9b9-49da-96c6-6760c63040ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b47ddf3-85c9-4dd8-bbbb-34fc3bd6aa1b",
   "metadata": {},
   "source": [
    "## GPT2ä½¿ç”¨peftæ ·ä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa3d240-44e1-4811-8f61-d6ff2500a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡, autodlä¸€èˆ¬åŒºåŸŸ\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bdb69d-3f0f-465e-bd60-2047a088e264",
   "metadata": {},
   "source": [
    "å¦‚æœæ‚¨ä¸ç¡®å®šæ¨¡å‹ä¸­æœ‰å“ªäº›æ¨¡å—å¯ä»¥å¾®è°ƒï¼Œå¯ä»¥æ‰“å°æ¨¡å‹ç»“æ„ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a0c049-9134-4d89-aad0-1aa2241a9fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4becc479adbc472bb7672d49da16aafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "transformer\n",
      "transformer.wte\n",
      "transformer.wpe\n",
      "transformer.drop\n",
      "transformer.h\n",
      "transformer.h.0\n",
      "transformer.h.0.ln_1\n",
      "transformer.h.0.attn\n",
      "transformer.h.0.attn.c_attn\n",
      "transformer.h.0.attn.c_proj\n",
      "transformer.h.0.attn.attn_dropout\n",
      "transformer.h.0.attn.resid_dropout\n",
      "transformer.h.0.ln_2\n",
      "transformer.h.0.mlp\n",
      "transformer.h.0.mlp.c_fc\n",
      "transformer.h.0.mlp.c_proj\n",
      "transformer.h.0.mlp.act\n",
      "transformer.h.0.mlp.dropout\n",
      "transformer.h.1\n",
      "transformer.h.1.ln_1\n",
      "transformer.h.1.attn\n",
      "transformer.h.1.attn.c_attn\n",
      "transformer.h.1.attn.c_proj\n",
      "transformer.h.1.attn.attn_dropout\n",
      "transformer.h.1.attn.resid_dropout\n",
      "transformer.h.1.ln_2\n",
      "transformer.h.1.mlp\n",
      "transformer.h.1.mlp.c_fc\n",
      "transformer.h.1.mlp.c_proj\n",
      "transformer.h.1.mlp.act\n",
      "transformer.h.1.mlp.dropout\n",
      "transformer.h.2\n",
      "transformer.h.2.ln_1\n",
      "transformer.h.2.attn\n",
      "transformer.h.2.attn.c_attn\n",
      "transformer.h.2.attn.c_proj\n",
      "transformer.h.2.attn.attn_dropout\n",
      "transformer.h.2.attn.resid_dropout\n",
      "transformer.h.2.ln_2\n",
      "transformer.h.2.mlp\n",
      "transformer.h.2.mlp.c_fc\n",
      "transformer.h.2.mlp.c_proj\n",
      "transformer.h.2.mlp.act\n",
      "transformer.h.2.mlp.dropout\n",
      "transformer.h.3\n",
      "transformer.h.3.ln_1\n",
      "transformer.h.3.attn\n",
      "transformer.h.3.attn.c_attn\n",
      "transformer.h.3.attn.c_proj\n",
      "transformer.h.3.attn.attn_dropout\n",
      "transformer.h.3.attn.resid_dropout\n",
      "transformer.h.3.ln_2\n",
      "transformer.h.3.mlp\n",
      "transformer.h.3.mlp.c_fc\n",
      "transformer.h.3.mlp.c_proj\n",
      "transformer.h.3.mlp.act\n",
      "transformer.h.3.mlp.dropout\n",
      "transformer.h.4\n",
      "transformer.h.4.ln_1\n",
      "transformer.h.4.attn\n",
      "transformer.h.4.attn.c_attn\n",
      "transformer.h.4.attn.c_proj\n",
      "transformer.h.4.attn.attn_dropout\n",
      "transformer.h.4.attn.resid_dropout\n",
      "transformer.h.4.ln_2\n",
      "transformer.h.4.mlp\n",
      "transformer.h.4.mlp.c_fc\n",
      "transformer.h.4.mlp.c_proj\n",
      "transformer.h.4.mlp.act\n",
      "transformer.h.4.mlp.dropout\n",
      "transformer.h.5\n",
      "transformer.h.5.ln_1\n",
      "transformer.h.5.attn\n",
      "transformer.h.5.attn.c_attn\n",
      "transformer.h.5.attn.c_proj\n",
      "transformer.h.5.attn.attn_dropout\n",
      "transformer.h.5.attn.resid_dropout\n",
      "transformer.h.5.ln_2\n",
      "transformer.h.5.mlp\n",
      "transformer.h.5.mlp.c_fc\n",
      "transformer.h.5.mlp.c_proj\n",
      "transformer.h.5.mlp.act\n",
      "transformer.h.5.mlp.dropout\n",
      "transformer.h.6\n",
      "transformer.h.6.ln_1\n",
      "transformer.h.6.attn\n",
      "transformer.h.6.attn.c_attn\n",
      "transformer.h.6.attn.c_proj\n",
      "transformer.h.6.attn.attn_dropout\n",
      "transformer.h.6.attn.resid_dropout\n",
      "transformer.h.6.ln_2\n",
      "transformer.h.6.mlp\n",
      "transformer.h.6.mlp.c_fc\n",
      "transformer.h.6.mlp.c_proj\n",
      "transformer.h.6.mlp.act\n",
      "transformer.h.6.mlp.dropout\n",
      "transformer.h.7\n",
      "transformer.h.7.ln_1\n",
      "transformer.h.7.attn\n",
      "transformer.h.7.attn.c_attn\n",
      "transformer.h.7.attn.c_proj\n",
      "transformer.h.7.attn.attn_dropout\n",
      "transformer.h.7.attn.resid_dropout\n",
      "transformer.h.7.ln_2\n",
      "transformer.h.7.mlp\n",
      "transformer.h.7.mlp.c_fc\n",
      "transformer.h.7.mlp.c_proj\n",
      "transformer.h.7.mlp.act\n",
      "transformer.h.7.mlp.dropout\n",
      "transformer.h.8\n",
      "transformer.h.8.ln_1\n",
      "transformer.h.8.attn\n",
      "transformer.h.8.attn.c_attn\n",
      "transformer.h.8.attn.c_proj\n",
      "transformer.h.8.attn.attn_dropout\n",
      "transformer.h.8.attn.resid_dropout\n",
      "transformer.h.8.ln_2\n",
      "transformer.h.8.mlp\n",
      "transformer.h.8.mlp.c_fc\n",
      "transformer.h.8.mlp.c_proj\n",
      "transformer.h.8.mlp.act\n",
      "transformer.h.8.mlp.dropout\n",
      "transformer.h.9\n",
      "transformer.h.9.ln_1\n",
      "transformer.h.9.attn\n",
      "transformer.h.9.attn.c_attn\n",
      "transformer.h.9.attn.c_proj\n",
      "transformer.h.9.attn.attn_dropout\n",
      "transformer.h.9.attn.resid_dropout\n",
      "transformer.h.9.ln_2\n",
      "transformer.h.9.mlp\n",
      "transformer.h.9.mlp.c_fc\n",
      "transformer.h.9.mlp.c_proj\n",
      "transformer.h.9.mlp.act\n",
      "transformer.h.9.mlp.dropout\n",
      "transformer.h.10\n",
      "transformer.h.10.ln_1\n",
      "transformer.h.10.attn\n",
      "transformer.h.10.attn.c_attn\n",
      "transformer.h.10.attn.c_proj\n",
      "transformer.h.10.attn.attn_dropout\n",
      "transformer.h.10.attn.resid_dropout\n",
      "transformer.h.10.ln_2\n",
      "transformer.h.10.mlp\n",
      "transformer.h.10.mlp.c_fc\n",
      "transformer.h.10.mlp.c_proj\n",
      "transformer.h.10.mlp.act\n",
      "transformer.h.10.mlp.dropout\n",
      "transformer.h.11\n",
      "transformer.h.11.ln_1\n",
      "transformer.h.11.attn\n",
      "transformer.h.11.attn.c_attn\n",
      "transformer.h.11.attn.c_proj\n",
      "transformer.h.11.attn.attn_dropout\n",
      "transformer.h.11.attn.resid_dropout\n",
      "transformer.h.11.ln_2\n",
      "transformer.h.11.mlp\n",
      "transformer.h.11.mlp.c_fc\n",
      "transformer.h.11.mlp.c_proj\n",
      "transformer.h.11.mlp.act\n",
      "transformer.h.11.mlp.dropout\n",
      "transformer.ln_f\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# æ‰“å°æ‰€æœ‰æ¨¡å—åç§°\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add2f79-f35c-4638-80bb-0d8a87a9b6a7",
   "metadata": {},
   "source": [
    "åœ¨é€‰æ‹© `target_modules` æ—¶ï¼Œé€šå¸¸ä¼šæ ¹æ®æ¨¡å—çš„åç§°é€‰æ‹©æ¨¡å‹çš„ç‰¹å®šéƒ¨åˆ†ï¼Œé€šå¸¸ä½¿ç”¨åˆ—è¡¨ä¸­æœ€åä¸€ä¸ªç‚¹ `.` åçš„å­—æ®µåæˆ–æ•´ä¸ªè·¯å¾„åï¼ˆå¦‚æœéœ€è¦æ›´ç²¾ç¡®ï¼‰ã€‚ä»¥ä¸‹æ˜¯å¯¹è¿™äº›æ¨¡å—çš„è¯¦ç»†åˆ†æå’Œé€‰æ‹©å»ºè®®ï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### **1. åˆ†ææ¨¡å—ç»“æ„**\n",
    "\n",
    "ä»åˆ—è¡¨ä¸­å¯ä»¥çœ‹å‡ºï¼ŒGPT-2 çš„æ¨¡å—å±‚æ¬¡åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š\n",
    "\n",
    "1. **Embedding å±‚**ï¼š\n",
    "   - `transformer.wte`ï¼šè¯åµŒå…¥å±‚ï¼ˆWord Token Embeddingsï¼‰ã€‚\n",
    "   - `transformer.wpe`ï¼šä½ç½®åµŒå…¥å±‚ï¼ˆPosition Embeddingsï¼‰ã€‚\n",
    "\n",
    "2. **Transformer ç¼–ç å™¨å±‚**ï¼š\n",
    "   - æ¯å±‚ç¼–å·ä¸º `transformer.h.<å±‚å·>`ï¼ˆå…± 12 å±‚ï¼‰ã€‚\n",
    "   - æ¯å±‚ä¸­åŒ…å«ï¼š\n",
    "     - **å±‚å½’ä¸€åŒ–**ï¼š\n",
    "       - `transformer.h.<å±‚å·>.ln_1`ï¼šç¬¬ä¸€å±‚å½’ä¸€åŒ–ã€‚\n",
    "       - `transformer.h.<å±‚å·>.ln_2`ï¼šç¬¬äºŒå±‚å½’ä¸€åŒ–ã€‚\n",
    "     - **è‡ªæ³¨æ„åŠ›æ¨¡å—**ï¼š\n",
    "       - `transformer.h.<å±‚å·>.attn.c_attn`ï¼šæ³¨æ„åŠ›æ¨¡å—çš„ Queryã€Key å’Œ Value æŠ•å½±ã€‚\n",
    "       - `transformer.h.<å±‚å·>.attn.c_proj`ï¼šæ³¨æ„åŠ›çš„è¾“å‡ºæŠ•å½±ã€‚\n",
    "       - `transformer.h.<å±‚å·>.attn.attn_dropout`ï¼šæ³¨æ„åŠ›çš„ Dropoutã€‚\n",
    "       - `transformer.h.<å±‚å·>.attn.resid_dropout`ï¼šæ®‹å·®çš„ Dropoutã€‚\n",
    "     - **å‰é¦ˆç½‘ç»œæ¨¡å—ï¼ˆMLPï¼‰**ï¼š\n",
    "       - `transformer.h.<å±‚å·>.mlp.c_fc`ï¼šMLP çš„ç¬¬ä¸€å±‚å…¨è¿æ¥ã€‚\n",
    "       - `transformer.h.<å±‚å·>.mlp.c_proj`ï¼šMLP çš„ç¬¬äºŒå±‚å…¨è¿æ¥ï¼ˆè¾“å‡ºæŠ•å½±ï¼‰ã€‚\n",
    "       - `transformer.h.<å±‚å·>.mlp.act`ï¼šæ¿€æ´»å‡½æ•°ï¼ˆå¦‚ GELUï¼‰ã€‚\n",
    "       - `transformer.h.<å±‚å·>.mlp.dropout`ï¼šMLP çš„ Dropoutã€‚\n",
    "\n",
    "3. **æœ€ç»ˆå±‚**ï¼š\n",
    "   - `transformer.ln_f`ï¼šæœ€ç»ˆå±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰ã€‚\n",
    "   - `lm_head`ï¼šè¯­è¨€å»ºæ¨¡å¤´ï¼Œç”¨äºç”Ÿæˆé¢„æµ‹çš„ token åˆ†å¸ƒã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **2. å¦‚ä½•é€‰æ‹© `target_modules`**\n",
    "\n",
    "#### **ï¼ˆ1ï¼‰å¸¸è§ç›®æ ‡æ¨¡å—**\n",
    "- `transformer.h.<å±‚å·>.attn.c_attn`ï¼šå¯¹è‡ªæ³¨æ„åŠ›æ¨¡å—çš„ Queryã€Key å’Œ Value æŠ•å½±å±‚å¾®è°ƒã€‚\n",
    "- `transformer.h.<å±‚å·>.attn.c_proj`ï¼šå¯¹æ³¨æ„åŠ›è¾“å‡ºçš„æŠ•å½±å±‚å¾®è°ƒã€‚\n",
    "- `transformer.h.<å±‚å·>.mlp.c_fc`ï¼šå¯¹å‰é¦ˆç½‘ç»œçš„è¾“å…¥å…¨è¿æ¥å±‚å¾®è°ƒã€‚\n",
    "- `transformer.h.<å±‚å·>.mlp.c_proj`ï¼šå¯¹å‰é¦ˆç½‘ç»œçš„è¾“å‡ºæŠ•å½±å±‚å¾®è°ƒã€‚\n",
    "\n",
    "#### **ï¼ˆ2ï¼‰æ¨èè®¾ç½®**\n",
    "- **æ–‡æœ¬ç”Ÿæˆä»»åŠ¡**ï¼š\n",
    "  ```python\n",
    "  target_modules = [\"transformer.h.*.attn.c_attn\", \"transformer.h.*.attn.c_proj\"]\n",
    "  ```\n",
    "  è§£é‡Šï¼š\n",
    "  - `*.attn.c_attn`ï¼šè°ƒæ•´ Queryã€Keyã€Value çš„ç”Ÿæˆã€‚\n",
    "  - `*.attn.c_proj`ï¼šè°ƒæ•´æ³¨æ„åŠ›è¾“å‡ºã€‚\n",
    "\n",
    "- **æ–‡æœ¬åˆ†ç±»ä»»åŠ¡**ï¼š\n",
    "  ```python\n",
    "  target_modules = [\"transformer.h.*.attn.c_attn\"]\n",
    "  ```\n",
    "  è§£é‡Šï¼š\n",
    "  - å¾®è°ƒè‡ªæ³¨æ„åŠ›æ¨¡å—æœ€é‡è¦çš„éƒ¨åˆ†å³å¯ã€‚\n",
    "\n",
    "- **ç‰¹å®šä»»åŠ¡éœ€è¦æ›´ç»†ç²’åº¦æ§åˆ¶**ï¼š\n",
    "  - ä»…å¾®è°ƒæŸå‡ å±‚ï¼š\n",
    "    ```python\n",
    "    target_modules = [\"transformer.h.0.attn.c_attn\", \"transformer.h.0.mlp.c_fc\"]\n",
    "    ```\n",
    "\n",
    "#### **ï¼ˆ3ï¼‰é€šé…ç¬¦é€‰æ‹©**\n",
    "ä½¿ç”¨ `*` é€šé…ç¬¦å¯ä»¥æŒ‡å®šæ‰€æœ‰å±‚çš„æŸäº›æ¨¡å—ï¼š\n",
    "- `transformer.h.*.attn.c_attn`ï¼šæ‰€æœ‰å±‚çš„ Queryã€Key å’Œ Value æŠ•å½±ã€‚\n",
    "- `transformer.h.*.mlp.*`ï¼šæ‰€æœ‰å±‚çš„ MLP æ¨¡å—ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **3. ç¤ºä¾‹ï¼šæŒ‡å®šå¤šä¸ªæ¨¡å—**\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"transformer.h.*.attn.c_attn\",\n",
    "        \"transformer.h.*.mlp.c_fc\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "```\n",
    "\n",
    "- è¿™è¡¨ç¤ºå¯¹æ‰€æœ‰å±‚çš„ `attn.c_attn` å’Œ `mlp.c_fc` æ¨¡å—è¿›è¡Œ LoRA å¾®è°ƒã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **4. å°æç¤ºï¼šå¦‚ä½•ç¡®å®šé€‚åˆçš„æ¨¡å—**\n",
    "\n",
    "1. **ä»»åŠ¡ç›¸å…³æ€§**ï¼š\n",
    "   - æ–‡æœ¬ç”Ÿæˆï¼šä¼˜å…ˆé€‰æ‹©è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼ˆå¦‚ `c_attn`ï¼‰ã€‚\n",
    "   - æ–‡æœ¬åˆ†ç±»ï¼šé€šå¸¸éœ€è¦å…¨å±€è¯­ä¹‰è¡¨ç¤ºï¼Œé€‰æ‹© `attn.c_attn` æˆ– `mlp.c_fc`ã€‚\n",
    "\n",
    "2. **æ€§èƒ½ä¸èµ„æºå¹³è¡¡**ï¼š\n",
    "   - å¦‚æœæ˜¾å­˜æœ‰é™ï¼Œå¯ä»¥åªå¾®è°ƒéƒ¨åˆ†å±‚ã€‚ä¾‹å¦‚ï¼Œä»…é€‰æ‹©æµ…å±‚å’Œæ·±å±‚çš„æ¨¡å—ï¼š\n",
    "     ```python\n",
    "     target_modules = [\"transformer.h.0.attn.c_attn\", \"transformer.h.11.attn.c_attn\"]\n",
    "     ```\n",
    "\n",
    "3. **æ‰“å°æ¨¡å—åç§°ä»¥è°ƒè¯•**ï¼š\n",
    "   - ç¡®ä¿é€‰æ‹©çš„ `target_modules` åœ¨æ¨¡å‹ä¸­å®é™…å­˜åœ¨ï¼š\n",
    "     ```python\n",
    "     for name, _ in model.named_modules():\n",
    "         if \"c_attn\" in name:\n",
    "             print(name)\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "### **å»ºè®®**\n",
    "- ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œ`c_attn` å’Œ `c_proj` æ˜¯é¦–é€‰æ¨¡å—ã€‚\n",
    "- ä½¿ç”¨ `transformer.h.*` é€šé…ç¬¦å¯ä»¥è½»æ¾æŒ‡å®šå¤šå±‚ã€‚\n",
    "- æ ¹æ®ä»»åŠ¡éœ€æ±‚å’Œèµ„æºé™åˆ¶çµæ´»è°ƒæ•´ç›®æ ‡æ¨¡å—ï¼Œä»¥å®ç°æœ€ä½³æ€§èƒ½å’Œæ•ˆç‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f20171-0719-4dfa-b888-147b657ebff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4e7bff2-2a4f-4a1d-9cb1-dd02aead2f85",
   "metadata": {},
   "source": [
    "## LoraConfigå…·ä½“é…ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c99eb9-8007-4297-972e-7be71768c9c3",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹æ˜¯å¯¹ `LoraConfig` é…ç½®çš„æ›´è¯¦ç»†è§£é‡Šï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•è®¾ç½®å¾®è°ƒå“ªäº›å‚æ•°ã€å†»ç»“å“ªäº›å‚æ•°ï¼Œä»¥åŠä¸€èˆ¬å¦‚ä½•é€‰æ‹©è¿™äº›è®¾ç½®ï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### **1. `LoraConfig` å‚æ•°è§£æ**\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # åºåˆ—åˆ†ç±»ä»»åŠ¡\n",
    "    r=8,                         # é™ä½çŸ©é˜µç§©\n",
    "    lora_alpha=32,               # LoRA çš„ alpha è¶…å‚æ•°\n",
    "    target_modules=[\"c_attn\"],   # GPT-2 ä¸­çš„è‡ªæ³¨æ„åŠ›æ¨¡å—\n",
    "    lora_dropout=0.1,            # dropout æ¦‚ç‡\n",
    "    bias=\"none\",                 # æ˜¯å¦å¾®è°ƒåç½®å‚æ•°\n",
    ")\n",
    "```\n",
    "\n",
    "#### **ï¼ˆ1ï¼‰`task_type`**\n",
    "- å®šä¹‰ä»»åŠ¡ç±»å‹ï¼Œç”¨äºæŒ‡å¯¼ PEFT çš„å…·ä½“è¡Œä¸ºã€‚\n",
    "- **å¸¸è§é€‰é¡¹**ï¼š\n",
    "  - `TaskType.CAUSAL_LM`ï¼šè‡ªå›å½’è¯­è¨€å»ºæ¨¡ï¼ˆå¦‚ GPT ç³»åˆ—æ¨¡å‹ï¼‰ã€‚\n",
    "  - `TaskType.SEQ_CLS`ï¼šåºåˆ—åˆ†ç±»ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æï¼‰ã€‚\n",
    "  - `TaskType.TOKEN_CLS`ï¼šæ ‡æ³¨ä»»åŠ¡ï¼ˆå¦‚å‘½åå®ä½“è¯†åˆ«ï¼‰ã€‚\n",
    "  - `TaskType.SEQ_2_SEQ_LM`ï¼šåºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼ˆå¦‚ç¿»è¯‘ã€æ‘˜è¦ï¼‰ã€‚\n",
    "\n",
    "**å½“å‰è®¾ç½®**ï¼š\n",
    "- `TaskType.SEQ_CLS` è¡¨ç¤ºç›®æ ‡æ˜¯æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### **ï¼ˆ2ï¼‰`r`**\n",
    "- è¡¨ç¤º LoRA çš„ **ç§©**ï¼ˆrankï¼‰ï¼Œæ˜¯é™ä½çŸ©é˜µç§©çš„æ ¸å¿ƒå‚æ•°ã€‚\n",
    "- LoRA é€šè¿‡å°†æ¨¡å‹çš„æƒé‡åˆ†è§£ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µï¼ˆ`A` å’Œ `B`ï¼‰ï¼Œåªæ›´æ–°è¿™ä¸¤ä¸ªçŸ©é˜µã€‚\n",
    "- `r` çš„å€¼è¶Šå¤§ï¼Œå¾®è°ƒèƒ½åŠ›è¶Šå¼ºï¼Œä½†éœ€è¦çš„é¢å¤–å‚æ•°ä¹Ÿè¶Šå¤šã€‚\n",
    "- **å…¸å‹èŒƒå›´**ï¼š`4` è‡³ `64`ï¼Œå¤§å¤šæ•°ä»»åŠ¡ä¸­ `8` æˆ– `16` æ˜¯å¸¸ç”¨å€¼ã€‚\n",
    "\n",
    "**å½“å‰è®¾ç½®**ï¼š\n",
    "- `r=8` è¡¨ç¤ºä½¿ç”¨ä½ç§©åˆ†è§£ï¼Œå¹¶å¾®è°ƒ 8 ç»´çš„å‚æ•°çŸ©é˜µã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### **ï¼ˆ3ï¼‰`lora_alpha`**\n",
    "- æ˜¯ LoRA çš„ä¸€ä¸ªç¼©æ”¾å› å­ï¼Œç”¨äºè°ƒèŠ‚ä¸¤ä¸ªä½ç§©çŸ©é˜µçš„æ›´æ–°é€Ÿç‡ã€‚\n",
    "- **å…¬å¼**ï¼šå®é™…æ›´æ–° = LoRA è¾“å‡º Ã— `lora_alpha / r`\n",
    "- **å…¸å‹èŒƒå›´**ï¼š`16` è‡³ `128`ï¼Œè¾ƒå¤§ä»»åŠ¡ä¸­å¯ä»¥é€‰æ‹©æ›´é«˜çš„å€¼ã€‚\n",
    "\n",
    "**å½“å‰è®¾ç½®**ï¼š\n",
    "- `lora_alpha=32`ï¼Œè¡¨ç¤ºé€‚ä¸­å¹…åº¦çš„æ›´æ–°é€Ÿç‡ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### **ï¼ˆ4ï¼‰`target_modules`**\n",
    "- æŒ‡å®šè¦åº”ç”¨ LoRA å¾®è°ƒçš„æ¨¡å—ã€‚\n",
    "- **å¸¸è§é€‰æ‹©**ï¼š\n",
    "  - å¯¹ Transformer æ¨¡å‹ä¸­çš„ **æ³¨æ„åŠ›æ¨¡å—**ï¼ˆå¦‚ `query`ã€`key`ã€`value`ï¼‰è¿›è¡Œå¾®è°ƒï¼Œå› ä¸ºè¿™äº›æ¨¡å—å¯¹ä»»åŠ¡æ€§èƒ½å½±å“è¾ƒå¤§ã€‚\n",
    "  - å¯¹ GPT-2ï¼Œé€šå¸¸é€‰æ‹© `c_attn`ï¼ˆGPT-2 ä¸­è´Ÿè´£è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç»„åˆæ¨¡å—ï¼‰ã€‚\n",
    "\n",
    "**å½“å‰è®¾ç½®**ï¼š\n",
    "- `target_modules=[\"c_attn\"]` è¡¨ç¤ºåªå¯¹ GPT-2 çš„è‡ªæ³¨æ„åŠ›æ¨¡å— `c_attn` åº”ç”¨ LoRAã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### **ï¼ˆ5ï¼‰`lora_dropout`**\n",
    "- è¡¨ç¤º LoRA å±‚çš„ dropout æ¦‚ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚\n",
    "- **å…¸å‹èŒƒå›´**ï¼š`0.0` è‡³ `0.1`ï¼Œè§†ä»»åŠ¡å¤æ‚æ€§è€Œå®šã€‚\n",
    "\n",
    "**å½“å‰è®¾ç½®**ï¼š\n",
    "- `lora_dropout=0.1`ï¼Œè¡¨ç¤ºæœ‰ 10% çš„æ¦‚ç‡éšæœºä¸¢å¼ƒ LoRA å±‚çš„è¾“å‡ºã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### **ï¼ˆ6ï¼‰`bias`**\n",
    "- å†³å®šæ˜¯å¦å¾®è°ƒåç½®å‚æ•°ã€‚\n",
    "- **é€‰é¡¹**ï¼š\n",
    "  - `\"none\"`ï¼šä¸å¾®è°ƒä»»ä½•åç½®ã€‚\n",
    "  - `\"all\"`ï¼šå¾®è°ƒæ‰€æœ‰åç½®ã€‚\n",
    "  - `\"lora_only\"`ï¼šåªå¾®è°ƒ LoRA å±‚çš„åç½®ã€‚\n",
    "\n",
    "**å½“å‰è®¾ç½®**ï¼š\n",
    "- `bias=\"none\"`ï¼Œè¡¨ç¤ºæ‰€æœ‰åç½®å‚æ•°ä¿æŒå†»ç»“ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **5. æ€»ç»“å»ºè®®**\n",
    "- **å¾®è°ƒçš„å‚æ•°**ï¼šä¼˜å…ˆé€‰æ‹©æ¨¡å‹ä¸­æ³¨æ„åŠ›ç›¸å…³æ¨¡å—ã€‚\n",
    "- **å†»ç»“çš„å‚æ•°**ï¼šå¤§éƒ¨åˆ†å‚æ•°é»˜è®¤å†»ç»“ä»¥èŠ‚çœæ˜¾å­˜ã€‚\n",
    "- **é…ç½®é€‰æ‹©**ï¼šæ ¹æ®ä»»åŠ¡å¤æ‚æ€§è°ƒæ•´ `r` å’Œ `target_modules`ã€‚\n",
    "- **æ¨èèµ·ç‚¹**ï¼š\n",
    "  - æ–‡æœ¬åˆ†ç±»ï¼š`target_modules=[\"c_attn\"]`, `r=8`, `lora_dropout=0.1`ã€‚\n",
    "  - æ–‡æœ¬ç”Ÿæˆï¼š`target_modules=[\"q_proj\", \"v_proj\"]`, `r=16`, `lora_dropout=0.1`ã€‚\n",
    "\n",
    "é€šè¿‡è¿™äº›è®¾ç½®ï¼ŒLoRA å¯ä»¥åœ¨å‚æ•°é‡æå°çš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆå¾®è°ƒï¼Œé€‚åˆå„ç§ä»»åŠ¡åœºæ™¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc080ba-3ee8-4bc6-afd9-2a3241f1bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡, autodlä¸€èˆ¬åŒºåŸŸ\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d9f362-18cc-471f-b208-f29a6933c06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at dnagpt/dna_gpt2_v0 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e72521368341d38a2b11028715a871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296,448 || all params: 109,180,416 || trainable%: 0.2715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# **1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨**\n",
    "model_name = \"dnagpt/dna_gpt2_v0\"  # åŸºç¡€æ¨¡å‹\n",
    "num_labels = 2       # äºŒåˆ†ç±»ä»»åŠ¡\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "# **2. å®šä¹‰æ•°æ®é›†**\n",
    "# ç¤ºä¾‹æ•°æ®é›†ï¼šdna_promoter_300\n",
    "dataset = load_dataset(\"dnagpt/dna_promoter_300\")['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# **3. æ•°æ®é¢„å¤„ç†**\n",
    "def preprocess_function(examples):\n",
    "    examples['label'] = [int(item) for item in examples['label']]\n",
    "    return tokenizer(\n",
    "        examples[\"sequence\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "#tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")  # Hugging Face Trainer è¦æ±‚æ ‡ç­¾åˆ—åä¸º 'labels'\n",
    "\n",
    "# 4. åˆ›å»ºä¸€ä¸ªæ•°æ®æ”¶é›†å™¨ï¼Œç”¨äºåŠ¨æ€å¡«å……å’Œé®è”½\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# **4. åˆ’åˆ†æ•°æ®é›†**\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# **5. é…ç½® LoRA**\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # åºåˆ—åˆ†ç±»ä»»åŠ¡\n",
    "    r=8,                         # é™ä½çŸ©é˜µç§©\n",
    "    lora_alpha=32,               # LoRA çš„ alpha è¶…å‚æ•°\n",
    "    target_modules=[\"c_attn\"],   # GPT-2 ä¸­çš„è‡ªæ³¨æ„åŠ›æ¨¡å—\n",
    "    lora_dropout=0.1,            # dropout æ¦‚ç‡\n",
    "    bias=\"none\",                 # æ˜¯å¦å¾®è°ƒåç½®å‚æ•°\n",
    ")\n",
    "\n",
    "# ä½¿ç”¨ LoRA åŒ…è£…æ¨¡å‹\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # æ‰“å°å¯è®­ç»ƒçš„å‚æ•°ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da39e7f-db92-483c-888d-19707ab35c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2399/742597822.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19980' max='19980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19980/19980 10:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.325549</td>\n",
       "      <td>0.897635</td>\n",
       "      <td>0.908117</td>\n",
       "      <td>0.885483</td>\n",
       "      <td>0.896658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.290004</td>\n",
       "      <td>0.904899</td>\n",
       "      <td>0.889069</td>\n",
       "      <td>0.925901</td>\n",
       "      <td>0.907111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.310100</td>\n",
       "      <td>0.289658</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.892138</td>\n",
       "      <td>0.924891</td>\n",
       "      <td>0.908219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³ ./gpt2_lora_text_classification\n"
     ]
    }
   ],
   "source": [
    "# **6. è®¡ç®—æŒ‡æ ‡**\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# **7. å®šä¹‰è®­ç»ƒå‚æ•°**\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_lora_text_classification\",  # æ¨¡å‹ä¿å­˜è·¯å¾„\n",
    "    evaluation_strategy=\"epoch\",                 # æ¯ä¸ª epoch è¯„ä¼°ä¸€æ¬¡\n",
    "    save_strategy=\"epoch\",                       # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡\n",
    "    learning_rate=2e-5,                          # å­¦ä¹ ç‡\n",
    "    per_device_train_batch_size=8,               # æ¯è®¾å¤‡çš„æ‰¹é‡å¤§å°\n",
    "    per_device_eval_batch_size=8,                # æ¯è®¾å¤‡è¯„ä¼°çš„æ‰¹é‡å¤§å°\n",
    "    num_train_epochs=10,                          # è®­ç»ƒè½®æ•°\n",
    "    weight_decay=0.01,                           # æƒé‡è¡°å‡\n",
    "    logging_dir=\"./logs\",                        # æ—¥å¿—è·¯å¾„\n",
    "    fp16=True,                                   # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ\n",
    "    save_total_limit=2,                          # ä¿ç•™æœ€å¤šä¸¤ä¸ªæ£€æŸ¥ç‚¹\n",
    "    load_best_model_at_end=True,                 # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    metric_for_best_model=\"accuracy\",            # æ ¹æ®å‡†ç¡®ç‡é€‰æ‹©æœ€ä½³æ¨¡å‹\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# **8. å®šä¹‰ Trainer**\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# **9. å¼€å§‹è®­ç»ƒ**\n",
    "trainer.train()\n",
    "\n",
    "# **10. ä¿å­˜æ¨¡å‹**\n",
    "model.save_pretrained(\"./gpt2_lora_text_classification\")\n",
    "tokenizer.save_pretrained(\"./gpt2_lora_text_classification\")\n",
    "\n",
    "print(\"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³ ./gpt2_lora_text_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a60fed-3a7d-4608-98b1-b4e313b94dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# åŠ è½½åˆ†è¯å™¨\n",
    "model_path = \"./gpt2_lora_text_classification\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# åŠ è½½å¾®è°ƒåçš„ PEFT æ¨¡å‹\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "model = PeftModel.from_pretrained(base_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d8f02-c3dc-4961-8b3a-50eefc5f9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict(texts, model, tokenizer):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨å¾®è°ƒåçš„ PEFT æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚\n",
    "    \n",
    "    Args:\n",
    "        texts (list of str): å¾…åˆ†ç±»çš„æ–‡æœ¬åˆ—è¡¨ã€‚\n",
    "        model (PeftModel): å¾®è°ƒåçš„æ¨¡å‹ã€‚\n",
    "        tokenizer (AutoTokenizer): åˆ†è¯å™¨ã€‚\n",
    "    \n",
    "    Returns:\n",
    "        list of dict: æ¯ä¸ªæ–‡æœ¬çš„é¢„æµ‹ç»“æœï¼ŒåŒ…æ‹¬ logits å’Œé¢„æµ‹çš„ç±»åˆ«æ ‡ç­¾ã€‚\n",
    "    \"\"\"\n",
    "    # å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯å’Œç¼–ç \n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°æ¨¡å‹çš„è®¾å¤‡ä¸Šï¼ˆCPU/GPUï¼‰\n",
    "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "    \n",
    "    # æ¨¡å‹æ¨ç†\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # è·å– logits å¹¶è®¡ç®—é¢„æµ‹ç±»åˆ«\n",
    "    logits = outputs.logits\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    predictions = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    # è¿”å›æ¯ä¸ªæ–‡æœ¬çš„é¢„æµ‹ç»“æœ\n",
    "    results = [\n",
    "        {\"text\": text, \"logits\": logit.tolist(), \"predicted_class\": int(pred)}\n",
    "        for text, logit, pred in zip(texts, logits, predictions)\n",
    "    ]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0cfe65-f4f3-4274-a4f4-1ac13725b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text: This movie was fantastic! I loved every part of it.\n",
    "Predicted Class: 1\n",
    "Logits: [-2.345, 3.567]\n",
    "\n",
    "Text: The plot was terrible and the acting was worse.\n",
    "Predicted Class: 0\n",
    "Logits: [4.123, -1.234]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
