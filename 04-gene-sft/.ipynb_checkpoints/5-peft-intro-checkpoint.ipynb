{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963e9ae0-ac68-44be-8c7d-fb9842784362",
   "metadata": {},
   "source": [
    "# 4.5 peft简介"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4288594-c676-4369-aca1-730446f293d7",
   "metadata": {},
   "source": [
    "## peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b82c4-d484-4c15-a600-03c3b51367ec",
   "metadata": {},
   "source": [
    "**PEFT**（Parameter-Efficient Fine-Tuning，参数高效微调）是一种优化技术，旨在以最小的参数更新实现对大规模预训练模型（如 GPT、BERT 等）的微调。PEFT 技术通过减少微调所需的参数量，显著降低了存储和计算开销，同时保留模型的性能，特别适合资源受限的场景和领域特定任务的定制化。\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 核心思想**\n",
    "传统的微调方式需要更新整个预训练模型的所有参数，PEFT 技术通过只调整少量的参数（如特定层或额外添加的小型模块）实现微调目标，大幅减少了训练开销和存储需求。\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 常见的 PEFT 方法**\n",
    "\n",
    "#### **（1）Adapter 模型**\n",
    "- 在每一层 Transformer 的输出中插入小型适配器模块，仅训练适配器模块的参数。\n",
    "- 原始模型参数保持冻结不变。\n",
    "- 优点：适配器模块参数量小，能适应不同任务。\n",
    "\n",
    "示例方法：\n",
    "- **AdapterFusion**\n",
    "- **MAD-X**\n",
    "\n",
    "---\n",
    "\n",
    "#### **（2）Prefix Tuning**\n",
    "- 在 Transformer 的输入前添加一组可学习的前缀向量，这些前缀与模型的注意力机制交互。\n",
    "- 只调整前缀向量的参数，而不更新原始模型。\n",
    "- 优点：对生成任务效果显著，参数量进一步减少。\n",
    "\n",
    "---\n",
    "\n",
    "#### **（3）LoRA（Low-Rank Adaptation）**\n",
    "- 将预训练模型中的部分权重分解为两个低秩矩阵，仅调整这些低秩矩阵的参数。\n",
    "- 原始权重保持冻结状态。\n",
    "- 优点：参数量极小，计算高效。\n",
    "  \n",
    "---\n",
    "\n",
    "#### **（4）Prompt Tuning**\n",
    "- 在输入文本中添加可学习的提示（Prompt）。\n",
    "- 适合 NLP 任务中的文本生成、分类等。\n",
    "- 优点：实现简单，易于集成到现有框架。\n",
    "\n",
    "---\n",
    "\n",
    "### **3. PEFT 的优势**\n",
    "\n",
    "1. **显著减少参数更新量**：\n",
    "   - 微调传统的大模型（如 GPT-3）需要更新数百亿参数，而 PEFT 仅需更新百万级别甚至更少的参数。\n",
    "\n",
    "2. **高效存储**：\n",
    "   - 每个任务的微调结果只需存储少量额外参数，而不是整个模型。\n",
    "\n",
    "3. **适用多任务**：\n",
    "   - 同一预训练模型可以通过不同的 PEFT 模块适配多个任务，无需重新训练。\n",
    "\n",
    "4. **降低计算开销**：\n",
    "   - 训练所需的内存和计算显著减少，适合资源有限的环境。\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 应用场景**\n",
    "\n",
    "1. **领域特定任务**：\n",
    "   - 医疗、法律、金融等领域微调预训练模型。\n",
    "\n",
    "2. **多任务学习**：\n",
    "   - 适配多个任务，复用同一模型的预训练权重。\n",
    "\n",
    "3. **资源受限场景**：\n",
    "   - 移动设备、边缘设备上的模型部署。\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Hugging Face PEFT 库**\n",
    "\n",
    "Hugging Face 提供了专门的 PEFT 库，支持多种参数高效微调技术：\n",
    "- **安装**：\n",
    "  ```bash\n",
    "  pip install peft\n",
    "  ```\n",
    "- **使用 LoRA 微调示例**：\n",
    "  ```python\n",
    "  from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "  from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "  # 加载模型和分词器\n",
    "  model_name = \"gpt2\"\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  # 配置 LoRA\n",
    "  lora_config = LoraConfig(\n",
    "      task_type=TaskType.CAUSAL_LM,\n",
    "      r=8,\n",
    "      lora_alpha=32,\n",
    "      target_modules=[\"q_proj\", \"v_proj\"],\n",
    "      lora_dropout=0.1,\n",
    "      bias=\"none\"\n",
    "  )\n",
    "\n",
    "  # 使用 LoRA 微调模型\n",
    "  model = get_peft_model(model, lora_config)\n",
    "  model.print_trainable_parameters()\n",
    "\n",
    "  # 微调代码...\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. PEFT 的局限性**\n",
    "1. **特定任务限制**：\n",
    "   - 在一些复杂任务中，PEFT 方法可能不如全量微调效果好。\n",
    "\n",
    "2. **需要设计合适的模块**：\n",
    "   - 不同任务需要选择和设计合适的 PEFT 技术。\n",
    "\n",
    "3. **与模型架构相关**：\n",
    "   - PEFT 技术可能需要对模型架构进行一定程度的修改。\n",
    "\n",
    "---\n",
    "\n",
    "### **7. 小结**\n",
    "PEFT 是一个极具潜力的技术，特别适合在有限资源下对大模型进行微调。它在许多领域和任务中已显示出良好的效果，例如 LoRA 和 Adapter 模型已经成为高效微调的主流方法。\n",
    "\n",
    "如果您需要实现高效微调，可以结合 Hugging Face 的 PEFT 库快速上手。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b2631-c9b9-49da-96c6-6760c63040ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b47ddf3-85c9-4dd8-bbbb-34fc3bd6aa1b",
   "metadata": {},
   "source": [
    "## GPT2使用peft样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa3d240-44e1-4811-8f61-d6ff2500a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "# 设置环境变量, autodl一般区域\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bdb69d-3f0f-465e-bd60-2047a088e264",
   "metadata": {},
   "source": [
    "如果您不确定模型中有哪些模块可以微调，可以打印模型结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a0c049-9134-4d89-aad0-1aa2241a9fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4becc479adbc472bb7672d49da16aafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "transformer\n",
      "transformer.wte\n",
      "transformer.wpe\n",
      "transformer.drop\n",
      "transformer.h\n",
      "transformer.h.0\n",
      "transformer.h.0.ln_1\n",
      "transformer.h.0.attn\n",
      "transformer.h.0.attn.c_attn\n",
      "transformer.h.0.attn.c_proj\n",
      "transformer.h.0.attn.attn_dropout\n",
      "transformer.h.0.attn.resid_dropout\n",
      "transformer.h.0.ln_2\n",
      "transformer.h.0.mlp\n",
      "transformer.h.0.mlp.c_fc\n",
      "transformer.h.0.mlp.c_proj\n",
      "transformer.h.0.mlp.act\n",
      "transformer.h.0.mlp.dropout\n",
      "transformer.h.1\n",
      "transformer.h.1.ln_1\n",
      "transformer.h.1.attn\n",
      "transformer.h.1.attn.c_attn\n",
      "transformer.h.1.attn.c_proj\n",
      "transformer.h.1.attn.attn_dropout\n",
      "transformer.h.1.attn.resid_dropout\n",
      "transformer.h.1.ln_2\n",
      "transformer.h.1.mlp\n",
      "transformer.h.1.mlp.c_fc\n",
      "transformer.h.1.mlp.c_proj\n",
      "transformer.h.1.mlp.act\n",
      "transformer.h.1.mlp.dropout\n",
      "transformer.h.2\n",
      "transformer.h.2.ln_1\n",
      "transformer.h.2.attn\n",
      "transformer.h.2.attn.c_attn\n",
      "transformer.h.2.attn.c_proj\n",
      "transformer.h.2.attn.attn_dropout\n",
      "transformer.h.2.attn.resid_dropout\n",
      "transformer.h.2.ln_2\n",
      "transformer.h.2.mlp\n",
      "transformer.h.2.mlp.c_fc\n",
      "transformer.h.2.mlp.c_proj\n",
      "transformer.h.2.mlp.act\n",
      "transformer.h.2.mlp.dropout\n",
      "transformer.h.3\n",
      "transformer.h.3.ln_1\n",
      "transformer.h.3.attn\n",
      "transformer.h.3.attn.c_attn\n",
      "transformer.h.3.attn.c_proj\n",
      "transformer.h.3.attn.attn_dropout\n",
      "transformer.h.3.attn.resid_dropout\n",
      "transformer.h.3.ln_2\n",
      "transformer.h.3.mlp\n",
      "transformer.h.3.mlp.c_fc\n",
      "transformer.h.3.mlp.c_proj\n",
      "transformer.h.3.mlp.act\n",
      "transformer.h.3.mlp.dropout\n",
      "transformer.h.4\n",
      "transformer.h.4.ln_1\n",
      "transformer.h.4.attn\n",
      "transformer.h.4.attn.c_attn\n",
      "transformer.h.4.attn.c_proj\n",
      "transformer.h.4.attn.attn_dropout\n",
      "transformer.h.4.attn.resid_dropout\n",
      "transformer.h.4.ln_2\n",
      "transformer.h.4.mlp\n",
      "transformer.h.4.mlp.c_fc\n",
      "transformer.h.4.mlp.c_proj\n",
      "transformer.h.4.mlp.act\n",
      "transformer.h.4.mlp.dropout\n",
      "transformer.h.5\n",
      "transformer.h.5.ln_1\n",
      "transformer.h.5.attn\n",
      "transformer.h.5.attn.c_attn\n",
      "transformer.h.5.attn.c_proj\n",
      "transformer.h.5.attn.attn_dropout\n",
      "transformer.h.5.attn.resid_dropout\n",
      "transformer.h.5.ln_2\n",
      "transformer.h.5.mlp\n",
      "transformer.h.5.mlp.c_fc\n",
      "transformer.h.5.mlp.c_proj\n",
      "transformer.h.5.mlp.act\n",
      "transformer.h.5.mlp.dropout\n",
      "transformer.h.6\n",
      "transformer.h.6.ln_1\n",
      "transformer.h.6.attn\n",
      "transformer.h.6.attn.c_attn\n",
      "transformer.h.6.attn.c_proj\n",
      "transformer.h.6.attn.attn_dropout\n",
      "transformer.h.6.attn.resid_dropout\n",
      "transformer.h.6.ln_2\n",
      "transformer.h.6.mlp\n",
      "transformer.h.6.mlp.c_fc\n",
      "transformer.h.6.mlp.c_proj\n",
      "transformer.h.6.mlp.act\n",
      "transformer.h.6.mlp.dropout\n",
      "transformer.h.7\n",
      "transformer.h.7.ln_1\n",
      "transformer.h.7.attn\n",
      "transformer.h.7.attn.c_attn\n",
      "transformer.h.7.attn.c_proj\n",
      "transformer.h.7.attn.attn_dropout\n",
      "transformer.h.7.attn.resid_dropout\n",
      "transformer.h.7.ln_2\n",
      "transformer.h.7.mlp\n",
      "transformer.h.7.mlp.c_fc\n",
      "transformer.h.7.mlp.c_proj\n",
      "transformer.h.7.mlp.act\n",
      "transformer.h.7.mlp.dropout\n",
      "transformer.h.8\n",
      "transformer.h.8.ln_1\n",
      "transformer.h.8.attn\n",
      "transformer.h.8.attn.c_attn\n",
      "transformer.h.8.attn.c_proj\n",
      "transformer.h.8.attn.attn_dropout\n",
      "transformer.h.8.attn.resid_dropout\n",
      "transformer.h.8.ln_2\n",
      "transformer.h.8.mlp\n",
      "transformer.h.8.mlp.c_fc\n",
      "transformer.h.8.mlp.c_proj\n",
      "transformer.h.8.mlp.act\n",
      "transformer.h.8.mlp.dropout\n",
      "transformer.h.9\n",
      "transformer.h.9.ln_1\n",
      "transformer.h.9.attn\n",
      "transformer.h.9.attn.c_attn\n",
      "transformer.h.9.attn.c_proj\n",
      "transformer.h.9.attn.attn_dropout\n",
      "transformer.h.9.attn.resid_dropout\n",
      "transformer.h.9.ln_2\n",
      "transformer.h.9.mlp\n",
      "transformer.h.9.mlp.c_fc\n",
      "transformer.h.9.mlp.c_proj\n",
      "transformer.h.9.mlp.act\n",
      "transformer.h.9.mlp.dropout\n",
      "transformer.h.10\n",
      "transformer.h.10.ln_1\n",
      "transformer.h.10.attn\n",
      "transformer.h.10.attn.c_attn\n",
      "transformer.h.10.attn.c_proj\n",
      "transformer.h.10.attn.attn_dropout\n",
      "transformer.h.10.attn.resid_dropout\n",
      "transformer.h.10.ln_2\n",
      "transformer.h.10.mlp\n",
      "transformer.h.10.mlp.c_fc\n",
      "transformer.h.10.mlp.c_proj\n",
      "transformer.h.10.mlp.act\n",
      "transformer.h.10.mlp.dropout\n",
      "transformer.h.11\n",
      "transformer.h.11.ln_1\n",
      "transformer.h.11.attn\n",
      "transformer.h.11.attn.c_attn\n",
      "transformer.h.11.attn.c_proj\n",
      "transformer.h.11.attn.attn_dropout\n",
      "transformer.h.11.attn.resid_dropout\n",
      "transformer.h.11.ln_2\n",
      "transformer.h.11.mlp\n",
      "transformer.h.11.mlp.c_fc\n",
      "transformer.h.11.mlp.c_proj\n",
      "transformer.h.11.mlp.act\n",
      "transformer.h.11.mlp.dropout\n",
      "transformer.ln_f\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 打印所有模块名称\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add2f79-f35c-4638-80bb-0d8a87a9b6a7",
   "metadata": {},
   "source": [
    "在选择 `target_modules` 时，通常会根据模块的名称选择模型的特定部分，通常使用列表中最后一个点 `.` 后的字段名或整个路径名（如果需要更精确）。以下是对这些模块的详细分析和选择建议：\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 分析模块结构**\n",
    "\n",
    "从列表中可以看出，GPT-2 的模块层次分为以下几类：\n",
    "\n",
    "1. **Embedding 层**：\n",
    "   - `transformer.wte`：词嵌入层（Word Token Embeddings）。\n",
    "   - `transformer.wpe`：位置嵌入层（Position Embeddings）。\n",
    "\n",
    "2. **Transformer 编码器层**：\n",
    "   - 每层编号为 `transformer.h.<层号>`（共 12 层）。\n",
    "   - 每层中包含：\n",
    "     - **层归一化**：\n",
    "       - `transformer.h.<层号>.ln_1`：第一层归一化。\n",
    "       - `transformer.h.<层号>.ln_2`：第二层归一化。\n",
    "     - **自注意力模块**：\n",
    "       - `transformer.h.<层号>.attn.c_attn`：注意力模块的 Query、Key 和 Value 投影。\n",
    "       - `transformer.h.<层号>.attn.c_proj`：注意力的输出投影。\n",
    "       - `transformer.h.<层号>.attn.attn_dropout`：注意力的 Dropout。\n",
    "       - `transformer.h.<层号>.attn.resid_dropout`：残差的 Dropout。\n",
    "     - **前馈网络模块（MLP）**：\n",
    "       - `transformer.h.<层号>.mlp.c_fc`：MLP 的第一层全连接。\n",
    "       - `transformer.h.<层号>.mlp.c_proj`：MLP 的第二层全连接（输出投影）。\n",
    "       - `transformer.h.<层号>.mlp.act`：激活函数（如 GELU）。\n",
    "       - `transformer.h.<层号>.mlp.dropout`：MLP 的 Dropout。\n",
    "\n",
    "3. **最终层**：\n",
    "   - `transformer.ln_f`：最终层归一化（LayerNorm）。\n",
    "   - `lm_head`：语言建模头，用于生成预测的 token 分布。\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 如何选择 `target_modules`**\n",
    "\n",
    "#### **（1）常见目标模块**\n",
    "- `transformer.h.<层号>.attn.c_attn`：对自注意力模块的 Query、Key 和 Value 投影层微调。\n",
    "- `transformer.h.<层号>.attn.c_proj`：对注意力输出的投影层微调。\n",
    "- `transformer.h.<层号>.mlp.c_fc`：对前馈网络的输入全连接层微调。\n",
    "- `transformer.h.<层号>.mlp.c_proj`：对前馈网络的输出投影层微调。\n",
    "\n",
    "#### **（2）推荐设置**\n",
    "- **文本生成任务**：\n",
    "  ```python\n",
    "  target_modules = [\"transformer.h.*.attn.c_attn\", \"transformer.h.*.attn.c_proj\"]\n",
    "  ```\n",
    "  解释：\n",
    "  - `*.attn.c_attn`：调整 Query、Key、Value 的生成。\n",
    "  - `*.attn.c_proj`：调整注意力输出。\n",
    "\n",
    "- **文本分类任务**：\n",
    "  ```python\n",
    "  target_modules = [\"transformer.h.*.attn.c_attn\"]\n",
    "  ```\n",
    "  解释：\n",
    "  - 微调自注意力模块最重要的部分即可。\n",
    "\n",
    "- **特定任务需要更细粒度控制**：\n",
    "  - 仅微调某几层：\n",
    "    ```python\n",
    "    target_modules = [\"transformer.h.0.attn.c_attn\", \"transformer.h.0.mlp.c_fc\"]\n",
    "    ```\n",
    "\n",
    "#### **（3）通配符选择**\n",
    "使用 `*` 通配符可以指定所有层的某些模块：\n",
    "- `transformer.h.*.attn.c_attn`：所有层的 Query、Key 和 Value 投影。\n",
    "- `transformer.h.*.mlp.*`：所有层的 MLP 模块。\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 示例：指定多个模块**\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"transformer.h.*.attn.c_attn\",\n",
    "        \"transformer.h.*.mlp.c_fc\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "```\n",
    "\n",
    "- 这表示对所有层的 `attn.c_attn` 和 `mlp.c_fc` 模块进行 LoRA 微调。\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 小提示：如何确定适合的模块**\n",
    "\n",
    "1. **任务相关性**：\n",
    "   - 文本生成：优先选择自注意力模块（如 `c_attn`）。\n",
    "   - 文本分类：通常需要全局语义表示，选择 `attn.c_attn` 或 `mlp.c_fc`。\n",
    "\n",
    "2. **性能与资源平衡**：\n",
    "   - 如果显存有限，可以只微调部分层。例如，仅选择浅层和深层的模块：\n",
    "     ```python\n",
    "     target_modules = [\"transformer.h.0.attn.c_attn\", \"transformer.h.11.attn.c_attn\"]\n",
    "     ```\n",
    "\n",
    "3. **打印模块名称以调试**：\n",
    "   - 确保选择的 `target_modules` 在模型中实际存在：\n",
    "     ```python\n",
    "     for name, _ in model.named_modules():\n",
    "         if \"c_attn\" in name:\n",
    "             print(name)\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "### **建议**\n",
    "- 一般情况下，`c_attn` 和 `c_proj` 是首选模块。\n",
    "- 使用 `transformer.h.*` 通配符可以轻松指定多层。\n",
    "- 根据任务需求和资源限制灵活调整目标模块，以实现最佳性能和效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f20171-0719-4dfa-b888-147b657ebff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4e7bff2-2a4f-4a1d-9cb1-dd02aead2f85",
   "metadata": {},
   "source": [
    "## LoraConfig具体配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c99eb9-8007-4297-972e-7be71768c9c3",
   "metadata": {},
   "source": [
    "以下是对 `LoraConfig` 配置的更详细解释，特别是如何设置微调哪些参数、冻结哪些参数，以及一般如何选择这些设置：\n",
    "\n",
    "---\n",
    "\n",
    "### **1. `LoraConfig` 参数解析**\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # 序列分类任务\n",
    "    r=8,                         # 降低矩阵秩\n",
    "    lora_alpha=32,               # LoRA 的 alpha 超参数\n",
    "    target_modules=[\"c_attn\"],   # GPT-2 中的自注意力模块\n",
    "    lora_dropout=0.1,            # dropout 概率\n",
    "    bias=\"none\",                 # 是否微调偏置参数\n",
    ")\n",
    "```\n",
    "\n",
    "#### **（1）`task_type`**\n",
    "- 定义任务类型，用于指导 PEFT 的具体行为。\n",
    "- **常见选项**：\n",
    "  - `TaskType.CAUSAL_LM`：自回归语言建模（如 GPT 系列模型）。\n",
    "  - `TaskType.SEQ_CLS`：序列分类（如情感分析）。\n",
    "  - `TaskType.TOKEN_CLS`：标注任务（如命名实体识别）。\n",
    "  - `TaskType.SEQ_2_SEQ_LM`：序列到序列任务（如翻译、摘要）。\n",
    "\n",
    "**当前设置**：\n",
    "- `TaskType.SEQ_CLS` 表示目标是文本分类任务。\n",
    "\n",
    "---\n",
    "\n",
    "#### **（2）`r`**\n",
    "- 表示 LoRA 的 **秩**（rank），是降低矩阵秩的核心参数。\n",
    "- LoRA 通过将模型的权重分解为两个低秩矩阵（`A` 和 `B`），只更新这两个矩阵。\n",
    "- `r` 的值越大，微调能力越强，但需要的额外参数也越多。\n",
    "- **典型范围**：`4` 至 `64`，大多数任务中 `8` 或 `16` 是常用值。\n",
    "\n",
    "**当前设置**：\n",
    "- `r=8` 表示使用低秩分解，并微调 8 维的参数矩阵。\n",
    "\n",
    "---\n",
    "\n",
    "#### **（3）`lora_alpha`**\n",
    "- 是 LoRA 的一个缩放因子，用于调节两个低秩矩阵的更新速率。\n",
    "- **公式**：实际更新 = LoRA 输出 × `lora_alpha / r`\n",
    "- **典型范围**：`16` 至 `128`，较大任务中可以选择更高的值。\n",
    "\n",
    "**当前设置**：\n",
    "- `lora_alpha=32`，表示适中幅度的更新速率。\n",
    "\n",
    "---\n",
    "\n",
    "#### **（4）`target_modules`**\n",
    "- 指定要应用 LoRA 微调的模块。\n",
    "- **常见选择**：\n",
    "  - 对 Transformer 模型中的 **注意力模块**（如 `query`、`key`、`value`）进行微调，因为这些模块对任务性能影响较大。\n",
    "  - 对 GPT-2，通常选择 `c_attn`（GPT-2 中负责自注意力机制的组合模块）。\n",
    "\n",
    "**当前设置**：\n",
    "- `target_modules=[\"c_attn\"]` 表示只对 GPT-2 的自注意力模块 `c_attn` 应用 LoRA。\n",
    "\n",
    "---\n",
    "\n",
    "#### **（5）`lora_dropout`**\n",
    "- 表示 LoRA 层的 dropout 概率，用于防止过拟合。\n",
    "- **典型范围**：`0.0` 至 `0.1`，视任务复杂性而定。\n",
    "\n",
    "**当前设置**：\n",
    "- `lora_dropout=0.1`，表示有 10% 的概率随机丢弃 LoRA 层的输出。\n",
    "\n",
    "---\n",
    "\n",
    "#### **（6）`bias`**\n",
    "- 决定是否微调偏置参数。\n",
    "- **选项**：\n",
    "  - `\"none\"`：不微调任何偏置。\n",
    "  - `\"all\"`：微调所有偏置。\n",
    "  - `\"lora_only\"`：只微调 LoRA 层的偏置。\n",
    "\n",
    "**当前设置**：\n",
    "- `bias=\"none\"`，表示所有偏置参数保持冻结。\n",
    "\n",
    "---\n",
    "\n",
    "### **5. 总结建议**\n",
    "- **微调的参数**：优先选择模型中注意力相关模块。\n",
    "- **冻结的参数**：大部分参数默认冻结以节省显存。\n",
    "- **配置选择**：根据任务复杂性调整 `r` 和 `target_modules`。\n",
    "- **推荐起点**：\n",
    "  - 文本分类：`target_modules=[\"c_attn\"]`, `r=8`, `lora_dropout=0.1`。\n",
    "  - 文本生成：`target_modules=[\"q_proj\", \"v_proj\"]`, `r=16`, `lora_dropout=0.1`。\n",
    "\n",
    "通过这些设置，LoRA 可以在参数量极小的情况下实现高效微调，适合各种任务场景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc080ba-3ee8-4bc6-afd9-2a3241f1bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "# 设置环境变量, autodl一般区域\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d9f362-18cc-471f-b208-f29a6933c06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at dnagpt/dna_gpt2_v0 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e72521368341d38a2b11028715a871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296,448 || all params: 109,180,416 || trainable%: 0.2715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# **1. 加载模型和分词器**\n",
    "model_name = \"dnagpt/dna_gpt2_v0\"  # 基础模型\n",
    "num_labels = 2       # 二分类任务\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "# **2. 定义数据集**\n",
    "# 示例数据集：dna_promoter_300\n",
    "dataset = load_dataset(\"dnagpt/dna_promoter_300\")['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# **3. 数据预处理**\n",
    "def preprocess_function(examples):\n",
    "    examples['label'] = [int(item) for item in examples['label']]\n",
    "    return tokenizer(\n",
    "        examples[\"sequence\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "#tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")  # Hugging Face Trainer 要求标签列名为 'labels'\n",
    "\n",
    "# 4. 创建一个数据收集器，用于动态填充和遮蔽\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# **4. 划分数据集**\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# **5. 配置 LoRA**\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # 序列分类任务\n",
    "    r=8,                         # 降低矩阵秩\n",
    "    lora_alpha=32,               # LoRA 的 alpha 超参数\n",
    "    target_modules=[\"c_attn\"],   # GPT-2 中的自注意力模块\n",
    "    lora_dropout=0.1,            # dropout 概率\n",
    "    bias=\"none\",                 # 是否微调偏置参数\n",
    ")\n",
    "\n",
    "# 使用 LoRA 包装模型\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # 打印可训练的参数信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da39e7f-db92-483c-888d-19707ab35c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2399/742597822.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19980' max='19980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19980/19980 10:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.325549</td>\n",
       "      <td>0.897635</td>\n",
       "      <td>0.908117</td>\n",
       "      <td>0.885483</td>\n",
       "      <td>0.896658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.290004</td>\n",
       "      <td>0.904899</td>\n",
       "      <td>0.889069</td>\n",
       "      <td>0.925901</td>\n",
       "      <td>0.907111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.310100</td>\n",
       "      <td>0.289658</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.892138</td>\n",
       "      <td>0.924891</td>\n",
       "      <td>0.908219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练完成，模型已保存至 ./gpt2_lora_text_classification\n"
     ]
    }
   ],
   "source": [
    "# **6. 计算指标**\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# **7. 定义训练参数**\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_lora_text_classification\",  # 模型保存路径\n",
    "    evaluation_strategy=\"epoch\",                 # 每个 epoch 评估一次\n",
    "    save_strategy=\"epoch\",                       # 每个 epoch 保存一次\n",
    "    learning_rate=2e-5,                          # 学习率\n",
    "    per_device_train_batch_size=8,               # 每设备的批量大小\n",
    "    per_device_eval_batch_size=8,                # 每设备评估的批量大小\n",
    "    num_train_epochs=10,                          # 训练轮数\n",
    "    weight_decay=0.01,                           # 权重衰减\n",
    "    logging_dir=\"./logs\",                        # 日志路径\n",
    "    fp16=True,                                   # 启用混合精度训练\n",
    "    save_total_limit=2,                          # 保留最多两个检查点\n",
    "    load_best_model_at_end=True,                 # 加载最佳模型\n",
    "    metric_for_best_model=\"accuracy\",            # 根据准确率选择最佳模型\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "# **8. 定义 Trainer**\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# **9. 开始训练**\n",
    "trainer.train()\n",
    "\n",
    "# **10. 保存模型**\n",
    "model.save_pretrained(\"./gpt2_lora_text_classification\")\n",
    "tokenizer.save_pretrained(\"./gpt2_lora_text_classification\")\n",
    "\n",
    "print(\"训练完成，模型已保存至 ./gpt2_lora_text_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a60fed-3a7d-4608-98b1-b4e313b94dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# 加载分词器\n",
    "model_path = \"./gpt2_lora_text_classification\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 加载微调后的 PEFT 模型\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "model = PeftModel.from_pretrained(base_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d8f02-c3dc-4961-8b3a-50eefc5f9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict(texts, model, tokenizer):\n",
    "    \"\"\"\n",
    "    使用微调后的 PEFT 模型进行推理。\n",
    "    \n",
    "    Args:\n",
    "        texts (list of str): 待分类的文本列表。\n",
    "        model (PeftModel): 微调后的模型。\n",
    "        tokenizer (AutoTokenizer): 分词器。\n",
    "    \n",
    "    Returns:\n",
    "        list of dict: 每个文本的预测结果，包括 logits 和预测的类别标签。\n",
    "    \"\"\"\n",
    "    # 对输入文本进行分词和编码\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 将输入数据移动到模型的设备上（CPU/GPU）\n",
    "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "    \n",
    "    # 模型推理\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 获取 logits 并计算预测类别\n",
    "    logits = outputs.logits\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    predictions = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    # 返回每个文本的预测结果\n",
    "    results = [\n",
    "        {\"text\": text, \"logits\": logit.tolist(), \"predicted_class\": int(pred)}\n",
    "        for text, logit, pred in zip(texts, logits, predictions)\n",
    "    ]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0cfe65-f4f3-4274-a4f4-1ac13725b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text: This movie was fantastic! I loved every part of it.\n",
    "Predicted Class: 1\n",
    "Logits: [-2.345, 3.567]\n",
    "\n",
    "Text: The plot was terrible and the acting was worse.\n",
    "Predicted Class: 0\n",
    "Logits: [4.123, -1.234]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
